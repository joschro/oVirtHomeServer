[hosts]
localhost

[disktype]
#@RAIDTYPE@ #Possible values raid6, raid10, raid5, jbod

[diskcount]
#@NUMBER_OF_DATA_DISKS@ #Ignored in case of jbod

[stripesize]
#@STRIPE_SIZE@ #256 in case of jbod

[script1]
action=execute
ignore_script_errors=no
file=/usr/share/gdeploy/scripts/grafton-sanity-check.sh -d /dev/mapper/HostVG

#[vdo] # Note: Uncomment if dedupe & compression needs to be enabled on device. Needs kmod-vdo module
#action=create
#names=@VDO_DEVICE_Name@
#devices=@DEVICE@
#logicalsize=@logical_size@T # Note:logicalsize is 10x physical space on disk
##slabsize=32G               # Note: used only when the physical size is few TBs
#blockmapcachesize=128M
#readcache=enabled
#readcachesize=20M
#emulate512=enabled
#writepolicy=auto

[pv]
#action=create
#devices=@DEVICE@ # Change to @VDO_DEVICE_name@ if using vdo

[vg1]
#action=create
#vgname=gluster_vg1
#pvname=@DEVICE@ # Change to @VDO_DEVICE_name@ if using vdo

[lv1]
action=create
vgname=HostVG
lvname=engine_lv
lvtype=thick
size=100GB
mount=/gluster_bricks/engine

[lv2]
action=create
vgname=HostVG
poolname=lvThinPool
lvtype=thinpool
poolmetadatasize=16GB
#size=@SIZE@ #For example: 18000GB, depending on device capacity. Units to be specified.

[lv3]
action=create
lvname=lv_vmdisks
poolname=lvThinPool
vgname=HostVG
lvtype=thinlv
mount=/gluster_bricks/vmstore
virtualsize=2000GB # Units to be specified, for instance 5000GB

[lv4]
action=create
lvname=lv_datadisks
poolname=lvThinPool
vgname=HostVG
lvtype=thinlv
mount=/gluster_bricks/data
virtualsize=1000GB # Units to be specified, for instance 5000GB

#[lv5]
#action=setup-cache
#ssd=@SSD_DEVICE@
#vgname=gluster_vg1
#poolname=lvthinpool
#cache_lv=lvcache
#cache_lvsize=5GB # Provide device size
## cachemode=writeback

[shell2]
action=execute
command=vdsm-tool configure --force

[script3]
action=execute
file=/usr/share/gdeploy/scripts/blacklist_all_disks.sh
ignore_script_errors=no

[selinux]
yes

[service3]
action=restart
service=glusterd
slice_setup=yes

[firewalld]
action=add
ports=111/tcp,2049/tcp,54321/tcp,5900/tcp,5900-6923/tcp,5666/tcp,16514/tcp,54322/tcp
services=glusterfs

[script2]
action=execute
file=/usr/share/gdeploy/scripts/disable-gluster-hooks.sh

[shell3]
action=execute
command=usermod -a -G gluster qemu

[volume]
action=create
volname=engine
transport=tcp
key=storage.owner-uid,storage.owner-gid,features.shard,performance.low-prio-threads,performance.strict-o-direct,network.remote-dio,network.ping-timeout,user.cifs,nfs.disable,performance.quick-read,performance.read-ahead,performance.io-cache,cluster.eager-lock
value=36,36,on,32,on,off,30,off,on,off,off,off,enable
brick_dirs=/gluster_bricks/engine/engine
ignore_volume_errors=no

[volume2]
action=create
volname=vmstore
transport=tcp
key=storage.owner-uid,storage.owner-gid,features.shard,performance.low-prio-threads,performance.strict-o-direct,network.remote-dio,network.ping-timeout,user.cifs,nfs.disable,performance.quick-read,performance.read-ahead,performance.io-cache,cluster.eager-lock
value=36,36,on,32,on,off,30,off,on,off,off,off,enable
brick_dirs=/gluster_bricks/vmstore/vmstore
ignore_volume_errors=no

[volume3]
action=create
volname=data
transport=tcp
key=storage.owner-uid,storage.owner-gid,features.shard,performance.low-prio-threads,performance.strict-o-direct,network.remote-dio,network.ping-timeout,user.cifs,nfs.disable,performance.quick-read,performance.read-ahead,performance.io-cache,cluster.eager-lock
value=36,36,on,32,on,off,30,off,on,off,off,off,enable
brick_dirs=/gluster_bricks/data/data
ignore_volume_errors=no
